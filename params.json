{"name":"Behrooz File System (BFS)","tagline":"An in-memory distributed POSIX-compatible file system","body":"### Welcome to BFS \r\nBFS is an open-source distributed, scalable, replicated in-memory filesystem. BFS provides a POSIX compatible interface and can be mounted under any directory and be used same as a traditional filesystem such as Ext4 or NTFS as a userspace application without any kernel modification. BFS utilizes memory to store data which means a blazingly fast read/write speed. BFS is backed by Openstack Swift object storage (backend storage can be a disk, Amazon S3, or any other persistent storage as well). BFS nodes can communicate thorough a TCP connection or a faster ZERO_Networking solution. In ZERO_Networking mode the regular operating system network stack is bypassed and raw packets are shared between userspace and kernel.\r\n\r\nBFS has been developed as part of Behrooz Shafiee Sarjaz master thesis under supervision of professor [Martin Karsten](https://cs.uwaterloo.ca/~mkarsten/) at [Cheriton School of Computer Science](https://cs.uwaterloo.ca/), [University of Waterloo](https://uwaterloo.ca/).\r\n\r\n\r\n### Preliminary evaluation\r\nBFS can be used as a standalone filesystem on a single machine (STANDALONE_MODE) or even as a replacement for Apache Hadoop HDFS. In order to evaluate BFS performance, three different sets of experiments were defined. First, BFS was evaluated using IOZONE filesystem benchmark tool in standalone mode in which all  \r\nIO operations are performed on a single machine. Second, BFS remote operations were evaluated using IOZONE; however, this time all IO operation were performed on a remote node. Finally, BFS was used as the underlying Hadoop mapreduce distributed file system.\r\n\r\n\r\n**Evaluation Environment**\r\n\r\n\r\nEvaluation environment includes a cluster of 7 machines each with the following specification:  \r\n\r\n* 2GB 266MHz DDR Memory \r\n* Intel(R) Xeon(TM) CPU 3.06GHz Quad core 32 bit processor \r\n* Intel 66MHz Gigabit Ethernet Controller \r\n* operated by Debian i686(32 bit) GNU/Linux 8.0 (Kernel 3.16.7)\r\n\r\n** Standalone mode **\r\nIn this mode BFS was evaluated by IOZONE for write, rewrite, read, reread, random read, and random write operations, all performed locally on a single node. Block size was varied from standard 4 KB to 16 MB with file size from 1MB to 1GB. Unfortunately, fuse library which is used by BFS to provide a transparent POSIX compatible interface to applications does not support block sizes larger than 128KB. Therefore, it was patched to support block sizes up to 16 MB. \r\n\r\n\r\n**Read on a single node locally:**\r\n\r\n\r\n![Read on a single node locally.](http://i.imgur.com/TWq1BLL.png)\r\n\r\n\r\n**Write on a single node locally:**\r\n\r\n\r\n![Write on a single node locally.](http://i.imgur.com/iVNg9e6.png)\r\n\r\nAs it can be seen from the graphs increasing block size improves IO rate; however, the peak  \r\nrate happens at 64KB and 128KB block size and does not increase anymore.\r\n\r\n** Network Mode **\r\nIn this mode, one of the cluster nodes was configured to zero byte available memory and the  \r\nrest of cluster nodes were configured with 70% of physical memory (1.3GB) as available  \r\nmemory. Then the node with zero byte available memory was instructed to run IOZONE  \r\nbenchmark; therefore, all of it’s operations were performed on remote nodes. In fact, even  \r\nthough the whole cluster was available only one other node would be used in this scenario because IOZONE performs all of its operations on one file;therefore, whoever is storing that  \r\nfile will perform IO operations instructed remotely by the zero byte node. \r\n\r\nBFS implements two modes of remote communications: TCP and ZERO networking. TCP  \r\nmode was implemented by a non­blocking socket model using select IO model with Poco C++ library. ZERO mode was implemented using PF_RING library. PF_RING is a high speed packet capture library which is usually used by packet filtering applications. The main benefit of PF_RING library is capturing and sending packets without going through kernel standard data structures. BFS utilizes PF_RING to communicate among nodes in the same subnet. Due to the fact that in a subnet with Ethernet Flow­Control enabled on all nodes and switches no packet is lost, BFS has no need for a reliable transport protocol. Thus, BFS implements only a minimal transport protocol capable of multiplexing and demultiplexing different tasks simultaneously (Similar to the concept of a socket).\r\n\r\n\r\n**Read remotely TCP:**\r\n\r\n\r\n![Read Remotely TCP](http://i.imgur.com/MNYOBbU.png)\r\n\r\n\r\n**Read Remotely ZERO_Networking:**\r\n\r\n\r\n![Read Remote Zero_Networking](http://i.imgur.com/9Uwootz.png)\r\n\r\n\r\n**Write remotely TCP:**\r\n\r\n\r\n![Write Remote TCP](http://i.imgur.com/kTxa76a.png)\r\n\r\n\r\n**Write Remotely ZERO_Networking:**\r\n\r\n\r\n![Read Remote Zero_Networking](http://i.imgur.com/GdowVeQ.png)\r\n\r\n\r\n\r\n** Hadoop Mapreduce DFSIO Benchmark **\r\n\r\n\r\nHadoop Mapreduce framework uses Hadoop Distributed File System(HDFS) as the default file system. HDFS is a highly distributed, replicated, scalable and portable file system written in JAVA. HDFS is accessed in mapreduce  applications using a standard set of libraries and it does not provide a POSIX interface;therefore, in order to use hdfs, developers need to first copy input data into hdfs and then after processing is finished copy back results from hdfs to the local filesystem. Moreover, HDFS does not support random reads and writes. Therefore,  \r\nusing BFS as hadoop filesystem provides many opportunities to improve the performance of mapreduce applications.  \r\n \r\nIn order to solve the problem of copying data back and forth, some parts of Hadoop Mapreduce framework need to be modified. However, the goal was not to modify the code and only hadoop configuration files were hacked to use BFS instead of HDFS.\r\n\r\nAfter replacing HDFS with BFS, hadoop DFSIO test was used to measure the improvements made by using BFS instead of HDFS. DFSIO test is a stress test originally developed by hadoop team to stress HDFS and find IO bottlenecks. DFSIO performs distributed writes and reads and reports the average IO rate. Benchmark inputs include number of files, size of each file, and block size.\r\n\r\nDFSIO test was performed 10 times using HDFS, BFS_TCP, and BFS_ZERO mode. The file size was set to 1 GB and number of files was set to 4.  \r\n\r\n\r\n** Write Test **\r\n\r\n\r\n![Write DFSIO](http://i.imgur.com/5T5YZPt.png)\r\n\r\nit can be seen from the graph that both BFS TCP and BFS ZERO perform significantly better than HDFS (10X faster) with a small error bar (std). This shows that all the engaging node performed write locally (in their memory). \r\n \r\n\r\n\r\n** Read test **\r\n\r\n\r\n![Read DFSIO](http://i.imgur.com/amD9U5F.png)\r\n\r\nThere is a wide range of Avg IO rate in the read test results from 20MB to 140MB per second. This is due to the fact that some of the engaged nodes read local data (in their memory) and some of them read data in the remote nodes. This can be proved by noticing the error bar on each of data points. For example when in BFS_ZERO the avg rate is 135MB/Sec the std is almost zero meaning that all the nodes read their data locally. Similarly, when the average rate is so small 25MB/Sec the std is also significantly small meaning that all the nodes performed their read remotely. Finally, in between average IO rates such as 50MB/Sec have the highest std because some of the nodes performed their reads locally and some remotely. \r\n\r\nAnother noticeable point about read was that even though the block size was set to 16MB, non of reads were performed in 16MB and the all used a very small varying block size from 4KB to around 512KB. This can explain the low IO rate when nodes read data remotely.\r\n\r\n### DISCLAIMER\r\nBFS is still under heavy development and is not recommended for production level use. You can use BFS at your own risk.\r\n\r\n### Authors and Contributors\r\nBFS has been developed as part of Behrooz Shafiee Sarjaz (@bshafiee) master thesis under supervision of professor [Martin Karsten](https://cs.uwaterloo.ca/~mkarsten/) at [Cheriton School of Computer Science](https://cs.uwaterloo.ca/), [University of Waterloo](https://uwaterloo.ca/).\r\n\r\n### Contact\r\nReach me at bshafiee@uwaterloo.ca \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}